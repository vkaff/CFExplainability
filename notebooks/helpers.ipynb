{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "from itertools import combinations\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StaticVars:\n",
    "    FLOAT_MAX = np.finfo(np.float32).max\n",
    "    INT_MAX = np.iinfo(np.int32).max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractionsInfo:\n",
    "    score = 0\n",
    "#     interactions = []\n",
    "#     complete_interactions = []\n",
    "#     iter_found = -1\n",
    "    y_loss = 1.0\n",
    "    proximity_loss = StaticVars.FLOAT_MAX\n",
    "#     total_loss = StaticVars.FLOAT_MAX\n",
    "\n",
    "    def __init__(self, uid, iid=-1, p=-1, fobj=True, fconstraint=True):\n",
    "        self.user_id = uid\n",
    "        self.item_id = iid\n",
    "\n",
    "        self.satisfy_objective = fobj\n",
    "        self.satisfy_contraints = fconstraint\n",
    "\n",
    "        self.recommendation = None\n",
    "        self.interactions = dict(original=None, initial=None, best=None)\n",
    "        self.loss = dict(initial=StaticVars.FLOAT_MAX, best=StaticVars.FLOAT_MAX)\n",
    "        self.iter_no = dict(initial=-1, best=-1, total=0)\n",
    "\n",
    "        self.solution_found = False\n",
    "        self.pos = StaticVars.INT_MAX\n",
    "        self.cfs_dist = 0\n",
    "\n",
    "    def __str__(self):\n",
    "        sorted_recommended_items = [\n",
    "            (n[0], n[1].detach().numpy().flatten()[0]) if isinstance(n[1], torch.Tensor)\n",
    "            else (n[0], n[1]) for n in self.recommendation\n",
    "        ]\n",
    "\n",
    "        return (f'\\n'\n",
    "                f'user_id: {self.user_id}, item_id: {self.item_id}\\n'\n",
    "                f'yloss: {round(self.y_loss, 4)}, proximity_loss: {int(self.proximity_loss)}\\n'\n",
    "                f'Item {self.item_id} is in position {self.pos} now!!!\\n'\n",
    "                f'Found in iteration {self.iter_no[\"best\"]} and the interacted items are {self.interactions[\"best\"]}\\n'\n",
    "                f'10-best recommended items {sorted_recommended_items}\\n')\n",
    "\n",
    "    def set_flags(self, do_objective, do_contraints):\n",
    "        self.satisfy_objective = do_objective\n",
    "        self.satisfy_contraints = do_contraints\n",
    "\n",
    "    def needs_update(self, loss):\n",
    "        if len(loss):\n",
    "            does_contraints = (not self.satisfy_contraints or self.y_loss > loss['yloss'])\n",
    "            does_objective = (not self.satisfy_objective or self.proximity_loss >= loss['proximity'])\n",
    "\n",
    "            if does_contraints and does_objective: return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def set_values(self, predictions, interacted_items, tot_interacted_items, loss, iter_no, k=10):\n",
    "\n",
    "        # get the ranking position of selected item in the list\n",
    "        rk_data = st.rankdata(-predictions, method='ordinal')\n",
    "        self.pos = rk_data[self.item_id]\n",
    "#         self.recommends = sorted(enumerate(predictions), key=lambda x: x[1], reverse=True)[:k]\n",
    "        accepted_preds = (rk_data <= k).nonzero()\n",
    "        self.recommends = sorted(\n",
    "            zip(predictions[accepted_preds], *accepted_preds), \n",
    "            key=lambda x: x[0], reverse=True)\n",
    "        self.iter_found = iter_no\n",
    "        self.y_loss = loss[0]\n",
    "        self.proximity_loss = loss[1]\n",
    "        self.interactions = interacted_items\n",
    "        self.complete_interactions = tot_interacted_items\n",
    "\n",
    "        self.solution_found = True\n",
    "\n",
    "    def update_values(self, predictions, ranking, interacted_items, original_input, loss, iter_no, k=10):\n",
    "        if ranking[self.item_id] > k and ranking[self.item_id] <= self.pos and loss < self.loss['best']:\n",
    "            # get the ranking position of selected item in the list\n",
    "            # rk_data = st.rankdata(-predictions, method='ordinal')\n",
    "            self.pos = ranking[self.item_id]\n",
    "    #         self.recommends = sorted(enumerate(predictions), key=lambda x: x[1], reverse=True)[:k]\n",
    "            accepted_preds = (ranking <= k).nonzero()\n",
    "            self.recommendation = sorted(\n",
    "                zip(predictions[accepted_preds], *accepted_preds),\n",
    "                key=lambda x: x[0], reverse=True)\n",
    "\n",
    "            self.iter_no['best'] = iter_no\n",
    "            self.loss['best'] = loss\n",
    "            self.interactions['best'] = interacted_items\n",
    "            if not self.solution_found:\n",
    "                self.iter_no['initial'] = iter_no\n",
    "                self.loss['initial'] = loss\n",
    "                self.interactions['initial'] = interacted_items\n",
    "\n",
    "            self.interactions['original'] = original_input\n",
    "            self.cfs_dist = len(self.interactions['original']) - len(self.interactions['best'])\n",
    "\n",
    "            self.solution_found = True\n",
    "\n",
    "        self.iter_no['total'] = iter_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_type='pooling', path='../models'):\n",
    "    ofile = f'{model_type}_model_1m_20interactions.pt'\n",
    "    return torch.load(os.path.join(path, ofile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_type='pooling', path='../models'):\n",
    "    ofile = f'{model_type}_model_1m_20interactions.pt'\n",
    "    return torch.save(model, os.path.join(path, ofile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def find_cfs(dataset, model, excluded_item_pos, no_users=None, max_allowed_permutations=None, top_k=10, total_CFs=1):\n",
    "    num_users = no_users or max(dataset.users_ids) + 1\n",
    "    max_perms = max_allowed_permutations or dataset.max_sequence_length\n",
    "\n",
    "    best_tot_loss_data = []\n",
    "    best_yloss_data = []\n",
    "\n",
    "    for user_id in trange(1, num_users):  # dataset.num_users):\n",
    "\n",
    "        seq_size = len(dataset.sequences[dataset.user_ids==user_id])\n",
    "        _total_loss = [None] * seq_size\n",
    "        _yloss = [None] * seq_size\n",
    "\n",
    "        for j in range(seq_size):    \n",
    "            if all(v > 0 for v in dataset.sequences[dataset.user_ids==user_id][j]):    \n",
    "                items_interacted = dataset.sequences[dataset.user_ids==user_id][j]\n",
    "                predictions = -model.predict(items_interacted)\n",
    "                predictions[items_interacted] = StaticVars.FLOAT_MAX\n",
    "\n",
    "                kth_item = predictions.argsort()[top_k - 1]\n",
    "                rec_item_exclude = predictions.argsort()[min(top_k, int(excluded_item_pos)) - 1]\n",
    "\n",
    "                _total_loss[j] = InteractionsInfo(user_id, rec_item_exclude)\n",
    "                _yloss[j] = InteractionsInfo(user_id, rec_item_exclude, fobj=False)\n",
    "\n",
    "                counter = 1        \n",
    "\n",
    "                for l in range(len(items_interacted) - 1, max(0, len(items_interacted) - max_perms), -1):\n",
    "                    if _total_loss[j].solution_found: break\n",
    "\n",
    "                    # produce permutations of various interactions\n",
    "                    perm = combinations(items_interacted, l)\n",
    "\n",
    "                    for i in perm:\n",
    "                        # predict next top-k items about to be selected        \n",
    "                        preds = model.predict(i)\n",
    "                        \n",
    "                        # convert logits produced by model, i.e., the probability distribution before normalization, \n",
    "                        # by using softmax\n",
    "                        tensor = torch.from_numpy(preds).float()\n",
    "                        preds = F.softmax(tensor, dim=0)\n",
    "\n",
    "                        yloss = compute_yloss(preds.numpy()[rec_item_exclude], preds.numpy()[kth_item], total_CFs)\n",
    "                        proximity_loss = compute_proximity_loss(np.asarray(i)[np.newaxis, :], items_interacted, total_CFs)\n",
    "                        \n",
    "                        # keep info about the best solution found depending on an objective function\n",
    "                        if _total_loss[j].needs_update(dict(yloss=yloss, proximity=proximity_loss)):                        \n",
    "                            _total_loss[j].set_values(\n",
    "                                preds, i, items_interacted, [yloss, proximity_loss], counter, top_k)\n",
    "                            \n",
    "#                         if _yloss[j].needs_update(dict(yloss=yloss, proximity=proximity_loss)):\n",
    "#                             _yloss[j].set_values(\n",
    "#                                 preds, i, items_interacted, [yloss, proximity_loss], counter, k)                 \n",
    "\n",
    "                        counter += 1 \n",
    "\n",
    "        best_tot_loss_data.append(_total_loss)\n",
    "        best_yloss_data.append(_yloss)\n",
    "        \n",
    "    return (best_tot_loss_data, best_yloss_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_embeddings_to_cosine_similarity_matrix(E):\n",
    "    \"\"\" \n",
    "    Converts a tensor of n embeddings to an (n, n) tensor of similarities.\n",
    "    \"\"\"\n",
    "    dot = E @ E.t()\n",
    "    norm = torch.norm(E, 2, 1)\n",
    "    x = torch.div(dot, norm)\n",
    "    x = torch.div(x, torch.unsqueeze(norm, -1))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "\n",
    "def embeddings_to_cosine_similarity_matrix(E):\n",
    "    \"\"\" \n",
    "    Converts a a tensor of n embeddings to an (n, n) tensor of similarities.\n",
    "    \"\"\"\n",
    "    similarities = [[cosine_similarity(a, b, dim=0) for a in E] for b in E]\n",
    "#     similarities = list(map(torch.cat, similarities))\n",
    "    similarities = list(map(lambda x: torch.stack(x, dim=-1), similarities))\n",
    "    return torch.stack(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "\n",
    "def compute_sim_matrix(dataset, metric='jaccard', adjusted=False):\n",
    "    # compute the item-item similarity matrix utilizing implicit feedback,\n",
    "    # i.e., whether interacted or not with an item\n",
    "\n",
    "    M = np.zeros((dataset.num_users, dataset.num_items), dtype=np.bool)\n",
    "    for u in trange(1, dataset.num_users):\n",
    "        np.add.at(\n",
    "            M[u], dataset.item_ids[dataset.user_ids == u],\n",
    "            dataset.ratings[dataset.user_ids == u]\n",
    "        )\n",
    "\n",
    "    if adjusted:\n",
    "        M_u = M.mean(axis=1)\n",
    "        M = M - M_u[:, np.newaxis]\n",
    "\n",
    "    similarity_matrix = 1 - squareform(pdist(M.T, metric))\n",
    "\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def rank_interactions_to_excluded_item_per_user(cfs, sims_matrix):\n",
    "    non_solvable_cases = []\n",
    "    total_data = []\n",
    "\n",
    "    for items in cfs:\n",
    "        for rec in items:\n",
    "            if rec is None: continue\n",
    "\n",
    "            if not rec.solution_found:\n",
    "                non_solvable_cases.append(rec.user_id)\n",
    "                continue\n",
    "\n",
    "            items_rank = st.rankdata(sims_matrix[rec.item_id, rec.complete_interactions])\n",
    "            similarity_rank = len(rec.complete_interactions) - items_rank + 1\n",
    "            del_items_indices = np.where(np.isin(\n",
    "                rec.complete_interactions, \n",
    "                list(set(rec.complete_interactions).difference(set(rec.interactions)))\n",
    "            ))\n",
    "            total_data.extend(sorted(similarity_rank[del_items_indices].astype(int)[-1:]))\n",
    "\n",
    "    return (Counter(total_data), non_solvable_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CFExplainability",
   "language": "python",
   "name": "cfexplainability"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
